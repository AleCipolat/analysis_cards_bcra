{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Install xrld dependency    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "import datetime as dt\n",
    "\n",
    "import plotly.express as px\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "from dash import html, dcc\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cred_deb_df = (\"C:\\\\Users\\\\Alejandro\\\\Desktop\\\\Alpha Project\\\\Code\\\\Credit_debit_report\\\\cred_deb.xls\")\n",
    "sheet_name = (\"cant_tarj_cred_deb\")\n",
    "bcra_features = ['Cantidad de tarjetas de crédito titulares', 'Cantidad de tarjetas de crédito adicionales', 'Cantidad de tarjetas de débito']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this dataset you could see that a series of steps had to be made in order to clean and use the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BcraData():\n",
    "\n",
    "    def __init__(self, excel_path: str, excel_tab: str, dataframe_names: list) -> object:\n",
    "        \"\"\"Extract the data from a csv file\n",
    "        \n",
    "        ### Inputs:\n",
    "            excel_path (str): The path of the selected data.\n",
    "            excel_tab (str): The excel tab from which the data will be obtained.\n",
    "            dataframe_names (list): A list with the names of the dataframes.       \n",
    "        \"\"\"\n",
    "\n",
    "        # Initizialing the variables.    \n",
    "        self.excel_path = excel_path\n",
    "        self.excel_tab = excel_tab\n",
    "        self.dataframe_names = dataframe_names        \n",
    "        \n",
    "        # A list for each transformational state.\n",
    "        self.listed_df = [] # Cleaned data with accumulative data\n",
    "        self.difference_df_list= [] # Cleaned data with differential data\n",
    "        self.stacked_df = [] # Stacked dataframes for each transformation.\n",
    "\n",
    "\n",
    "    def get_data(self) -> pd.DataFrame: # get_data_frame\n",
    "        \"\"\"gets the data in a pandas datafarme format normalized and cleaned\"\"\"\n",
    "        \n",
    "        self.__extraction_transform()\n",
    "        \n",
    "        self.__accumulative_data()\n",
    "        \n",
    "        self.__calculate_difference()\n",
    "        \n",
    "        self.__merge_dfs()\n",
    "\n",
    "        return self.merged_df\n",
    "\n",
    "\n",
    "    # Dataframe extraction and cleaning process.\n",
    "    def __extraction_transform(self)-> list:\n",
    "        \"\"\"For each feature to obtain it creates a dataframe from the original excel file.\"\"\"\n",
    "        \n",
    "        for self.dataframe in self.dataframe_names:\n",
    "            self.__data_frame_extraction()  # Extraction of the DataFrame from the excel file.\n",
    "            self.__data_frame_cleaning()    # Cleans the datafarme from the non useful rows and columns.\n",
    "            self.__columns_clean_rename()   # Renames the default columns names into the real ones.    \n",
    "            self.__set_date_time()          # Transform the datafarme into a time series.\n",
    "        \n",
    "            self.listed_df.append(self.extracted_df)   # aggregates every Dataframe generated into a list.\n",
    "\n",
    "        return self.listed_df\n",
    "\n",
    "\n",
    "\n",
    "    # Dataframe extraction\n",
    "    def __data_frame_extraction(self) -> pd.DataFrame:  \n",
    "        \"\"\"Takes the dataset from the excel file and transform it into a pandas DataFrame\n",
    "        # Inputs:\n",
    "        * excel_path (str): The path of the selected data.\n",
    "        * excel_tab (str): The excel tab from which the data will be obtained.\n",
    "        \"\"\"\n",
    "\n",
    "        self.extracted_df = pd.read_excel(self.excel_path,\n",
    "                                         sheet_name = self.excel_tab,\n",
    "                                         skiprows = 17,\n",
    "                                         engine = \"xlrd\",\n",
    "                                         header = [0,2,3,4])\n",
    "\n",
    "        self.extracted_df = self.extracted_df.dropna(axis=0)\n",
    "        \n",
    "        return self.extracted_df\n",
    "\n",
    "\n",
    "\n",
    "    def __data_frame_cleaning(self) -> list:\n",
    "        \"\"\" Cleans the dataframe and remove the non useful elements\n",
    "        \"\"\"\n",
    "              \n",
    "        self.extracted_df = self.extracted_df[[\"Año\", \"Mes\", self.dataframe]]\n",
    "        # Slicing the data and droping the non useful levels.\n",
    "        self.extracted_df = self.extracted_df.iloc[1:]\n",
    "        self.extracted_df = self.extracted_df.droplevel(axis=1, level=[0,1,3])\n",
    "        self.extracted_df = self.extracted_df.drop(columns=self.extracted_df.iloc[:, 0].name)\n",
    "\n",
    "        return self.extracted_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __columns_clean_rename(self) -> list:\n",
    "        \"\"\"It's cleans and rename the columns\"\"\"\n",
    "        self.extracted_df = self.extracted_df.rename(columns={self.extracted_df.iloc[:, 0].name:\"Mes\",\n",
    "                                                              self.extracted_df.iloc[:, 2].name:\"Total\",\n",
    "                                                              self.extracted_df.iloc[:, 9].name:\"Compañías financieras\",\n",
    "                                                              self.extracted_df.iloc[:, 10].name:\"Cajas de crédito\",\n",
    "                                                              self.extracted_df.iloc[:, 1].name:\"Año mes\"})\n",
    "        \n",
    "        return self.extracted_df\n",
    "\n",
    "\n",
    "\n",
    "    def __set_date_time(self) -> list:\n",
    "        \"\"\" Transform the extracted data into a Time Series.\"\"\"\n",
    "        \n",
    "        # transform into string every element in the column and split the follwings elements.\n",
    "        month_list = list(self.extracted_df[\"Año mes\"].values)\n",
    "        month_strings = [str(months) for months in month_list]\n",
    "        month_strings = [re.split(\"\\.\", months) for months in month_strings]\n",
    "\n",
    "        # Creates a list from the months and years.\n",
    "        month = [months[1] for months in month_strings]\n",
    "        year = [years[0] for years in month_strings]\n",
    "    \n",
    "        # Creates the following \n",
    "        self.extracted_df[\"Month\"] = month\n",
    "        self.extracted_df[\"Year\"] = year\n",
    "        self.extracted_df[\"date\"] = self.extracted_df[\"Month\"] + \"/\" + self.extracted_df[\"Year\"]\n",
    "        self.extracted_df[\"date\"] = pd.to_datetime(self.extracted_df[\"date\"], format = \"%m/%Y\") + pd.offsets.MonthEnd()\n",
    "\n",
    "        # set index an drop the not useful columns\n",
    "        self.extracted_df = self.extracted_df.set_index(\"date\")\n",
    "        self.extracted_df = self.extracted_df.drop(columns=[\"Mes\", \"Año mes\", \"Month\", \"Year\"])        \n",
    "\n",
    "        return self.extracted_df\n",
    "    \n",
    "    \n",
    "    # Accumulative data\n",
    "    def __accumulative_data(self):\n",
    "        self.__stacking_dfs(self.listed_df)\n",
    "        self.accumulative_df = self.stacked_joined_df\n",
    "        \n",
    "        return self.accumulative_df\n",
    "\n",
    "\n",
    "    # Transform the accumulative data into differential period by period observational data.\n",
    "    def __calculate_difference(self) -> list:\n",
    "        \"\"\"it's transform from the data provided which shows an accumulative observations for each date into the difference within each one\"\"\"  \n",
    "        \n",
    "        for dataframe in range(len(self.listed_df)):\n",
    "            self.columns_names = list(self.listed_df[dataframe].columns)\n",
    "            self.listed_df[dataframe] = self.listed_df[dataframe].replace([\"…\"], 0)\n",
    "            self.listed_df[dataframe] = self.listed_df[dataframe].astype(\"int32\")            \n",
    "            self.difference_df = self.listed_df[dataframe].diff(periods=1)\n",
    "            \n",
    "            for column in self.columns_names:\n",
    "                self.difference_df[column] = self.difference_df[column].fillna(self.listed_df[dataframe].loc[self.listed_df[dataframe][column].index[0], column])\n",
    "\n",
    "            self.difference_df_list.append(self.difference_df)\n",
    "        \n",
    "        self.__stacking_dfs(self.difference_df_list)\n",
    "        \n",
    "        self.difference_df = self.stacked_joined_df\n",
    "        self.difference_df = self.difference_df.rename(columns={\"cantidad_titulares\":\"cantidad_titulares_var\"})\n",
    "\n",
    "        return self.difference_df\n",
    "\n",
    "\n",
    "    def __merge_dfs(self):\n",
    "        \n",
    "        self.accumulative_df = self.accumulative_df.reset_index()\n",
    "        self.difference_df = self.difference_df.reset_index()\n",
    "        \n",
    "        \n",
    "        self.merged_df = pd.merge(left = self.accumulative_df,\n",
    "                                  right = self.difference_df,\n",
    "                                  how=\"left\"\n",
    "                                  )\n",
    "        self.merged_df = self.merged_df.set_index(\"date\")\n",
    "        \n",
    "        return self.merged_df\n",
    "    \n",
    "\n",
    "    # Stacks the features into one\n",
    "    def __stacking_dfs(self, list_dataframe:list) -> pd.DataFrame.stack:\n",
    "        \n",
    "        self.stacked_dfs = []\n",
    "\n",
    "        for dataframe in range(len(list_dataframe)):\n",
    "            \n",
    "            list_dataframe[dataframe] = list_dataframe[dataframe].replace([\"…\"], 0)\n",
    "            list_dataframe[dataframe] = list_dataframe[dataframe].astype(\"int32\")\n",
    "            \n",
    "            df = list_dataframe[dataframe].stack().to_frame()\n",
    "            df = df.rename(columns={0: self.dataframe_names[dataframe]})\n",
    "\n",
    "            self.stacked_dfs.append(df)\n",
    "        self.__join_dfs()\n",
    "        self.__stack_features()\n",
    "        \n",
    "        return self.stacked_joined_df\n",
    "\n",
    "\n",
    "    def __join_dfs(self) -> pd.DataFrame.join:\n",
    "        self.stacked_dfs = self.stacked_dfs[0].join(self.stacked_dfs[1:])\n",
    "        self.stacked_dfs[self.stacked_dfs.columns] = self.stacked_dfs[self.stacked_dfs.columns].replace([\"…\"], 0)\n",
    "        self.stacked_dfs = self.stacked_dfs[self.stacked_dfs.columns].astype(int)\n",
    "        self.stacked_dfs = self.stacked_dfs.reset_index()\n",
    "        self.stacked_dfs = self.stacked_dfs.set_index([\"date\",\"level_1\"])\n",
    "\n",
    "        return self.stacked_dfs\n",
    "\n",
    "    def __stack_features(self) -> pd.DataFrame.stack:\n",
    "        self.stacked_dfs = self.stacked_dfs.stack().reset_index()\n",
    "        self.stacked_dfs = self.stacked_dfs.set_index(\"date\")\n",
    "        self.stacked_dfs = self.stacked_dfs.rename(columns={\"level_1\":\"entidades_financieras\",\n",
    "                                                          \"level_2\":\"instrumento\",\n",
    "                                                          0:\"cantidad_titulares\"})\n",
    "                                \n",
    "        self.stacked_dfs[\"instrumento\"] = self.stacked_dfs[\"instrumento\"].str.split(\"Cantidad de \", expand=True)[1]\n",
    "        \n",
    "        self.stacked_joined_df = self.stacked_dfs\n",
    "\n",
    "        return self.stacked_joined_df\n",
    "\n",
    "    \n",
    "    # Load into an csv file.\n",
    "    def generate_csv(self, file_name:str) -> csv:\n",
    "        \n",
    "        self.merged_df.to_csv(file_name+\".csv\", sep=\";\", encoding='utf-8-sig')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcra_data = BcraData(excel_path = path_cred_deb_df, excel_tab = sheet_name, dataframe_names = bcra_features)\n",
    "\n",
    "tarj_df = bcra_data.get_data()\n",
    "bcra_data.generate_csv(\"final_cards\")\n",
    "\n",
    "\n",
    "tarj_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2be2693fe4cde51120fd16a333efe4da08ce82ff7ce5e11a4bf338a4e2f815c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
